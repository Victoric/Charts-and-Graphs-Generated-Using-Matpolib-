# -*- coding: utf-8 -*-
"""BETH_Dataset_Analysis_With_ML1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y05A04OR_wVLCV4BZ5mm8iF5hEX2uS--

#Step 1 Importing necessary Python libraries such as pandas, numpy, seaborn, and matplotlib
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""# Step 2 Importing dataset and unzipping the 'archive.zip' file to access the data"""

# Importing the zipfile module
import zipfile

# Creating a ZipFile object with the file name
zip_file = zipfile.ZipFile("/content/archive.zip")

# Extracting all the files in the current directory
zip_file.extractall()

# Closing the ZipFile object
zip_file.close()

"""# Loading the 'labelled_testing_data.csv' file into a pandas DataFrame"""

#beth_df_1 = pd.read_csv("labelled_training_data.csv")
import pandas as pd
beth_df_1=pd.read_csv('/content/labelled_testing_data.csv',  encoding='ISO-8859-1')
beth_df_1.head()

"""# Loading the 'labelled_training_data.csv' and 'labelled_validation_data.csv' files into pandas DataFrames"""

#beth_df_2 = pd.read_csv("labelled_testing_data.csv")
#beth_df_3 = pd.read_csv("labelled_validation_data.csv")
import pandas as pd
beth_df_2=pd.read_csv('/content/labelled_training_data.csv',  encoding='ISO-8859-1')
beth_df_2.head()
beth_df_3=pd.read_csv('/content/labelled_validation_data.csv',  encoding='ISO-8859-1')
beth_df_3.head()



"""# Checking the shape (number of rows and columns) of the three DataFrames"""

beth_df_1.shape

beth_df_2.shape

beth_df_3.shape

"""# Merging the three DataFrames into one and check the shape of the merged DataFrame"""

#Merge Dataframe
beth_df = pd.concat ([beth_df_1, beth_df_2, beth_df_3])
beth_df.shape

"""# Shuffling the merged DataFrame and resetting the index"""

#shuffle Dataframe
df = beth_df.sample(frac=1).reset_index(drop=True)

"""# Displaying the first few rows of the DataFrame"""

df.head(2)





"""# STEP 3 Exploring the Dataset

# Using a heatmap to visualize the presence of any null values in the DataFrame
"""

#check null value in the dataset
plt.figure(figsize = (10,10))
sns.heatmap(df.isnull(), cbar = False, cmap="YlGnBu")

"""# Dropping the columns 'parentProcessId', 'userId', 'mountNamespace', and 'argsNum' from the DataFrame"""

new = df.drop(['parentProcessId','userId','mountNamespace','argsNum'], axis = 1)

"""# Displaying the first few rows of the DataFrame after dropping columns"""

new.head(5)

"""# Visualizing the pairwise relationships in the DataFrame using a pair plot"""

sns.pairplot(new)



"""# Displaying information about the DataFrame including the index dtype and column dtypes, non-null values, and memory usage"""

df.info()

"""# Displaying descriptive statistics of the DataFrame that summarize the central tendency, dispersion, and shape of a dataset's distribution"""

df.describe()







"""#STEP 4 ENCODING

# Encoding the 'hostName' column of the DataFrame using label encoding
"""

#Target column encoding
def encode_text_index(df,name):
  le = preprocessing.LabelEncoder()
  df[name] = le.fit_transform(df[name])
  return le.classes_

"""# Normalizing the numeric columns in the DataFrame using z-score normalization"""

#Target column encoding
from sklearn import preprocessing
#def encode_text_index(df,name):
le = preprocessing.LabelEncoder()
le.fit_transform(df['hostName'])
le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
print(le_name_mapping)

#Encoding the numeric column
def  encode_numeric_zscore(df, name, mean=None, sd=None):
  if mean is None:
    mean= df[name].mean()

  if sd is None:
    sd=df[name].std()

  df[name] = (df[name] - mean) / sd

encode_numeric_zscore(df,'mountNamespace')
encode_numeric_zscore(df,'threadId')
encode_numeric_zscore(df,'processId')
encode_numeric_zscore(df,'timestamp')
encode_numeric_zscore(df,'parentProcessId')
encode_numeric_zscore(df,'eventId')
encode_numeric_zscore(df,'returnValue')

hostNmae = encode_text_index(df,'hostName')
df.head()

df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.fillna(0, inplace=True)





"""#STEP 5 Training Testing Spliting

# Copying the 'evil' column from the DataFrame into a new variable 'y
"""

y=df[['evil']].copy()

"""# Selecting the 'timestamp', 'processId', 'threadId', 'eventId', and 'returnValue' columns for feature selection"""

feature_selection=['timestamp','processId','threadId','eventId','returnValue']

X=df[feature_selection].copy()

"""# Splittinng the data into training and testing sets using a test size of 0.22 and a random state of 350"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=350)

"""# Installing the 'joblib' library and importing it"""

!pip install joblib
import joblib



"""# Training a Random Forest Classifier on the training data and save the model using 'joblib'."""

from sklearn.ensemble import RandomForestClassifier

RF_classifier = RandomForestClassifier(n_estimators = 40, criterion = 'entropy', max_leaf_nodes=30, random_state = 350)
RF_classifier.fit(X_train, y_train)

# Save the model
joblib.dump(RF_classifier, "/content/RF_classifier.pkl")

from sklearn.metrics import classification_report, confusion_matrix

y_predict_train = RF_classifier.predict(X_train)
y_predict_train
#cm = confusion_matrix(y_train, y_predict_train)
#sns.heatmap(cm, annot=True)

# Predicting the Test set results
y_predict_test = RF_classifier.predict(X_test)
#cm = confusion_matrix(y_test, y_predict_test)
#sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))





"""# Training a Gradient Boosting Classifier on the training data and save the model using 'joblib"""

from sklearn.ensemble import GradientBoostingClassifier

GBDT_classifier = GradientBoostingClassifier()
GBDT_classifier.fit(X_train, y_train)

# Save the model
joblib.dump(GBDT_classifier, "/content/GBDT_classifier.pkl")

from sklearn.metrics import classification_report, confusion_matrix

y_predict_train = GBDT_classifier.predict(X_train)
y_predict_train
#cm = confusion_matrix(y_train, y_predict_train)
#sns.heatmap(cm, annot=True)

# Predicting the Test set results
y_predict_test = GBDT_classifier.predict(X_test)
#cm = confusion_matrix(y_test, y_predict_test)
#sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))





"""# Training an XGBoost Classifier on the training data and save the model using 'joblib'."""

from xgboost import XGBClassifier

XGB_classifier = XGBClassifier()
XGB_classifier.fit(X_train, y_train)

# Save the model
joblib.dump(XGB_classifier, "/content/XGB_classifier.pkl")

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_predict_train = XGB_classifier.predict(X_train)
y_predict_train
#cm = confusion_matrix(y_train, y_predict_train)
#sns.heatmap(cm, annot=True)

# Predicting the Test set results
y_predict_test = XGB_classifier.predict(X_test)
#cm = confusion_matrix(y_test, y_predict_test)
#sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))





"""# Training a K-Nearest Neighbors Classifier on the training data and save the model using 'joblib'"""

from sklearn.neighbors import KNeighborsClassifier

neigh = KNeighborsClassifier()
neigh.fit(X_train, y_train)

# Save the model
joblib.dump(neigh, "/content/KNN_classifier.pkl")

from sklearn.metrics import classification_report, confusion_matrix

y_predict_train = neigh.predict(X_train)
y_predict_train

y_predict_test = neigh.predict(X_test)

print(classification_report(y_test, y_predict_test))





"""# Training a Naive Bayes Classifier on the training data and save the model using 'joblib'"""

from sklearn.naive_bayes import GaussianNB

neigh = GaussianNB()
neigh.fit(X_train, y_train)

# Save the model
joblib.dump(neigh, "/content/GaussianNB_classifier.pkl")

from sklearn.metrics import classification_report, confusion_matrix

y_predict_train = neigh.predict(X_train)
y_predict_train

y_predict_test = neigh.predict(X_test)

print(classification_report(y_test, y_predict_test))





"""# Traing a Decision Tree Classifier on the training data and save the model using 'joblib'"""

from sklearn.tree import DecisionTreeClassifier

DT_classifier = DecisionTreeClassifier(max_leaf_nodes=30, random_state = 350)
DT_classifier.fit(X_train, y_train)

# Save the model
joblib.dump(DT_classifier, "/content/DT_classifier.pkl")

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_predict_train = DT_classifier.predict(X_train)
y_predict_train
#cm = confusion_matrix(y_train, y_predict_train)
#sns.heatmap(cm, annot=True)

# Predicting the Test set results
y_predict_test = DT_classifier.predict(X_test)
#cm = confusion_matrix(y_test, y_predict_test)
#sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))





"""# Training an AdaBoost Classifier on the training data and save the model using 'joblib'"""

from sklearn.ensemble import AdaBoostClassifier

AB_classifier = AdaBoostClassifier(n_estimators = 40,  random_state = 350)
AB_classifier.fit(X_train, y_train)

# Save the model
joblib.dump(AB_classifier, "/content/AB_classifier.pkl")

from sklearn.metrics import classification_report, confusion_matrix

y_predict_train = AB_classifier.predict(X_train)
y_predict_train

y_predict_test = AB_classifier.predict(X_test)

print(classification_report(y_test, y_predict_test))





"""# Plotting ROC curves for the Random Forest, Gradient Boosting, XGBoost, Decision Tree, and Naive Bayes classifiers and calculating the AUC for each"""

import matplotlib.pyplot as plt
from sklearn import metrics

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
XGB_classifier = XGBClassifier()

#set up plotting area
plt.figure(0).clf()

#fit Random Forest model and plot ROC curve
model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
auc = round(metrics.roc_auc_score(y_test, y_pred), 4)
plt.plot(fpr,tpr,label="Random Forest, AUC="+str(auc))

#fit Gradient Boosting model and plot ROC curve
model = GradientBoostingClassifier()
model.fit(X_train, y_train)
y_pred = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
auc = round(metrics.roc_auc_score(y_test, y_pred), 4)
plt.plot(fpr,tpr,label="Gradient Boosting, AUC="+str(auc))

#fit XGBoost model and plot ROC curve
model = XGBClassifier()
model.fit(X_train, y_train)
y_pred = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
auc = round(metrics.roc_auc_score(y_test, y_pred), 4)
plt.plot(fpr,tpr,label="XGBoost, AUC="+str(auc))

#fit DT model and plot ROC curve
model = DT_classifier
model.fit(X_train, y_train)
y_pred = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
auc = round(metrics.roc_auc_score(y_test, y_pred), 4)
plt.plot(fpr,tpr,label="DT, AUC="+str(auc))


#fit NAIVE BAYES model and plot ROC curve

model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
auc = round(metrics.roc_auc_score(y_test, y_pred), 4)
plt.plot(fpr,tpr,label="NAIVE BAYES, AUC="+str(auc))



# Plot line with no predictive power (baseline)
plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--', label='Baseline')

#add legend
plt.legend()